"""
Quick fix: Only regenerate pseudo points with full-year dates (includes wet season).
This is MUCH faster than rebuilding everything (2.5h vs 5h).

Only processes pseudo points (is_fire=False), keeping all fire points unchanged.
"""
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import logging
from datetime import datetime
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

sys.path.append(str(Path(__file__).parent.parent))

from features.pipeline_optimized import OptimizedFeaturePipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def extract_features_worker(args):
    """Worker function for parallel feature extraction."""
    idx, row, pipeline = args
    try:
        features = pipeline.extract_all_features(
            row['lat'], row['lon'], row['dtw_start'], row['dtw_end']
        )
        features['idx'] = idx
        features['is_fire'] = row['is_fire']
        return features
    except Exception as e:
        logger.error(f"Error extracting features for point {idx}: {e}")
        return {'idx': idx, 'error': str(e), 'is_fire': row.get('is_fire', None)}

def fix_pseudo_points():
    """Fix pseudo points by resampling dates from full year range."""
    logger.info("=" * 80)
    logger.info("QUICK FIX: Regenerating pseudo points with full-year dates")
    logger.info("=" * 80)
    
    # Load existing feature matrix
    feature_file = "data/processed/features_5000.parquet"
    logger.info(f"Loading existing feature matrix from {feature_file}")
    
    if not Path(feature_file).exists():
        logger.error(f"Feature file not found: {feature_file}")
        logger.error("Please run build_features_5000.py first!")
        return False
    
    df = pd.read_parquet(feature_file)
    df['date'] = pd.to_datetime(df['date'])
    
    logger.info(f"Total points: {len(df)}")
    logger.info(f"Fire points: {(df['is_fire'] == True).sum()}")
    logger.info(f"Pseudo points: {(df['is_fire'] == False).sum()}")
    
    # Separate fire and pseudo points
    fire_points = df[df['is_fire'] == True].copy()
    pseudo_points = df[df['is_fire'] == False].copy()
    
    logger.info(f"\n‚úÖ Keeping {len(fire_points)} fire points unchanged")
    logger.info(f"üîÑ Regenerating {len(pseudo_points)} pseudo points with full-year dates")
    
    # Get full year range from fire points
    min_year = fire_points['date'].dt.year.min()
    max_year = fire_points['date'].dt.year.max()
    full_year_start = pd.Timestamp(min_year, 1, 1)
    full_year_end = pd.Timestamp(max_year, 12, 31)
    
    logger.info(f"\nüìÖ Resampling pseudo point dates from: {full_year_start.date()} to {full_year_end.date()}")
    logger.info("‚ö†Ô∏è  This includes wet season (Jul-Sep) to prevent seasonal leakage")
    
    # CRITICAL FIX: Use same date range as fire points (only have dry season data)
    # BUT distribute evenly across months to prevent temporal leakage
    logger.info("\nüîÑ Resampling dates for pseudo points...")
    logger.info("‚ö†Ô∏è  Using same date range as fire points (no wet season data available)")
    logger.info("‚ö†Ô∏è  But distributing evenly across all months in that range")
    
    fire_start = fire_points['date'].min()
    fire_end = fire_points['date'].max()
    
    # Get all unique months in fire date range
    fire_months = sorted(fire_points['date'].dt.month.unique())
    logger.info(f"Fire date range: {fire_start.date()} to {fire_end.date()}")
    logger.info(f"Months with fire data: {fire_months}")
    
    # Distribute pseudo points evenly across months
    points_per_month = len(pseudo_points) // len(fire_months)
    remaining = len(pseudo_points) % len(fire_months)
    
    new_dates = []
    np.random.seed(42)  # For reproducibility
    
    for i, month in enumerate(fire_months):
        # Get dates in this month from fire points
        month_dates = fire_points[fire_points['date'].dt.month == month]['date'].unique()
        
        # Number of points for this month
        count = points_per_month + (1 if i < remaining else 0)
        
        # Randomly sample dates from this month (with replacement if needed)
        sampled_dates = np.random.choice(month_dates, size=count, replace=True)
        new_dates.extend(sampled_dates)
    
    # Shuffle to randomize order
    np.random.shuffle(new_dates)
    pseudo_points['date'] = pd.to_datetime(new_dates[:len(pseudo_points)])
    
    # Show date distribution
    months = pseudo_points['date'].dt.month
    logger.info(f"\n‚úÖ Pseudo point date distribution (evenly distributed):")
    for month in sorted(months.unique()):
        count = (months == month).sum()
        month_name = pd.Timestamp(2020, month, 1).strftime('%B')
        season = "Dry" if month in [3, 4, 5, 6] else ("Wet" if month in [7, 8, 9] else "Post-monsoon")
        logger.info(f"  {month_name:12} ({month:2d}): {count:5d} points ({count/len(pseudo_points)*100:5.1f}%) - {season}")
    
    # CRITICAL FIX: Calculate DTW properly (same method as fire points)
    # This prevents the model from learning "random DTW = no fire" vs "proper DTW = fire"
    logger.info("\nüîÑ Calculating PROPER DTW windows for pseudo points (precipitation-based)...")
    logger.info("‚ö†Ô∏è  This uses the SAME method as fire points, not random windows")
    
    from features.dtw import DynamicTimeWindow
    dtw = DynamicTimeWindow(thcp=30.0, thdp=10.0, max_window_days=90)
    chirps_dir = 'data/raw/chirps'
    
    dtw_starts = []
    dtw_ends = []
    valid_count = 0
    
    for idx, row in tqdm(pseudo_points.iterrows(), total=len(pseudo_points), desc="DTW Calculation"):
        try:
            # Use SAME DTW calculation as fire points (precipitation-based)
            dtw_start, dtw_end = dtw.calculate_dtw(
                fire_date=row['date'],
                lat=row['lat'],
                lon=row['lon'],
                chirps_dir=chirps_dir
            )
            dtw_starts.append(dtw_start)
            dtw_ends.append(dtw_end)
            valid_count += 1
        except Exception as e:
            # Fallback to simple window if DTW calculation fails
            logger.debug(f"DTW calculation failed for pseudo point {idx}: {e}")
            dtw_days = np.random.randint(7, 30)  # Use longer fallback window
            dtw_start = row['date'] - pd.Timedelta(days=dtw_days)
            dtw_starts.append(dtw_start)
            dtw_ends.append(row['date'])
    
    pseudo_points['dtw_start'] = dtw_starts
    pseudo_points['dtw_end'] = dtw_ends
    
    logger.info(f"‚úÖ DTW calculated successfully for {valid_count}/{len(pseudo_points)} pseudo points")
    
    # Re-extract features for pseudo points only
    logger.info("\nüîÑ Re-extracting features for pseudo points (parallel processing)...")
    pipeline = OptimizedFeaturePipeline()
    
    feature_names = [
        'elevation', 'slope', 'aspect',
        'ndvi_min', 'ndvi_median', 'ndvi_mean', 'ndvi_max',
        'evi_min', 'evi_median', 'evi_mean', 'evi_max',
        'ndwi_min', 'ndwi_median', 'ndwi_mean', 'ndwi_max',
        'prec_min', 'prec_median', 'prec_mean', 'prec_max', 'prec_sum',
        'lst_min', 'lst_median', 'lst_mean', 'lst_max'
    ]
    
    # Prepare tasks for parallel processing
    n_jobs = min(6, cpu_count() - 2)
    logger.info(f"Using {n_jobs} parallel workers")
    tasks = [(idx, row, pipeline) for idx, row in pseudo_points.iterrows()]
    
    results = []
    with Pool(processes=n_jobs) as pool:
        with tqdm(total=len(pseudo_points), desc="Feature Extraction", unit="points") as pbar:
            chunksize = max(10, 200 // n_jobs)
            for result in pool.imap_unordered(extract_features_worker, tasks, chunksize=chunksize):
                results.append(result)
                pbar.update(1)
    
    # Convert results to DataFrame
    logger.info("\nüîÑ Processing results...")
    results_df = pd.DataFrame(results)
    if 'idx' in results_df.columns:
        results_df = results_df.set_index('idx')
    
    # Ensure all feature columns exist
    for col in feature_names + ['is_fire']:
        if col not in results_df.columns:
            results_df[col] = np.nan
    
    # Reconstruct pseudo points feature matrix
    pseudo_feature_matrix = results_df[feature_names + ['is_fire']].copy()
    
    # Add metadata columns
    metadata_cols = ['date', 'lat', 'lon', 'dtw_start', 'dtw_end']
    for col in metadata_cols:
        if col in pseudo_points.columns:
            pseudo_feature_matrix[col] = pseudo_feature_matrix.index.map(
                lambda idx: pseudo_points.loc[idx, col] if idx in pseudo_points.index else (pd.NaT if 'date' in col or 'dtw' in col else np.nan)
            )
    
    # Reconstruct fire points feature matrix (keep original)
    logger.info("\nüîÑ Combining with original fire points...")
    fire_feature_matrix = fire_points[feature_names + ['is_fire'] + metadata_cols].copy()
    
    # Combine back
    fixed_feature_matrix = pd.concat([fire_feature_matrix, pseudo_feature_matrix], ignore_index=True)
    
    # Ensure is_fire is boolean
    fixed_feature_matrix['is_fire'] = fixed_feature_matrix['is_fire'].astype(bool)
    
    logger.info(f"\n‚úÖ Fixed feature matrix: {len(fixed_feature_matrix)} points")
    logger.info(f"   Fire points: {(fixed_feature_matrix['is_fire'] == True).sum()}")
    logger.info(f"   Pseudo points: {(fixed_feature_matrix['is_fire'] == False).sum()}")
    
    # Save backup of original
    backup_file = feature_file.replace('.parquet', '_backup_before_seasonal_fix.parquet')
    logger.info(f"\nüíæ Saving backup to {backup_file}")
    df.to_parquet(backup_file)
    
    # Save fixed version
    logger.info(f"üíæ Saving fixed feature matrix to {feature_file}")
    fixed_feature_matrix.to_parquet(feature_file)
    
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ FIX COMPLETE!")
    logger.info("=" * 80)
    logger.info("\nüéØ What was fixed:")
    logger.info("  1. ‚úÖ Pseudo points distributed evenly across all months (prevents temporal leakage)")
    logger.info("  2. ‚úÖ DTW calculated properly for pseudo points (same method as fire points)")
    logger.info("  3. ‚úÖ Both classes now have similar feature distributions")
    logger.info("\nüìã Next steps:")
    logger.info("  1. Delete old model to force retraining:")
    logger.info("     rm models/model.joblib")
    logger.info("     (or: del models\\model.joblib on Windows)")
    logger.info("  2. Retrain model:")
    logger.info("     python scripts/build_features_5000.py")
    logger.info("  3. Re-evaluate model")
    logger.info("  4. Expected scores: F1: 0.70-0.85 (was 0.99), AUC: 0.75-0.90 (was 0.998)")
    logger.info("\n‚ö†Ô∏è  Why this works:")
    logger.info("  - Both fire and non-fire points now span the same months")
    logger.info("  - Both use the same DTW calculation method")
    logger.info("  - Model must learn ACTUAL fire conditions, not artifacts:")
    logger.info("    * Not 'this month = fire'")
    logger.info("    * Not 'random DTW = no fire'")
    logger.info("    * But 'these precipitation/vegetation/LST patterns = fire'")
    
    return True

if __name__ == "__main__":
    success = fix_pseudo_points()
    if not success:
        sys.exit(1)

